{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# MNIST CNN with PyTorch (torchvision.datasets.MNIST)\n\n**Author:** Auto-generated by M365 Copilot  \n**Date:** 2025-12-18\n\nThis notebook implements a **basic Convolutional Neural Network (CNN)** on the **MNIST** handwritten digits dataset using **PyTorch** and **torchvision**. It includes:\n\n- Clean, well-commented training & evaluation code\n- Plots for loss and accuracy\n- Confusion matrix visualization\n- Notes and practical guidance on **hyperparameter tuning**\n- (Optional) a small hyperparameter sweep snippet\n\n> Reference: torchvision MNIST dataset docs — https://docs.pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Setup\n\nRun the following cell **once** if PyTorch/torchvision are not installed in your environment. If you run in Google Colab, uncomment the pip installs.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# If needed, uncomment to install\n# !pip install --upgrade pip\n# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# For CUDA-enabled machines (pick the correct version for your GPU)\n# See: https://pytorch.org/get-started/locally/\n# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Imports & Configuration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\nimport random\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\n\n# Ensure plots render inline\n%matplotlib inline\n\nprint('PyTorch version:', torch.__version__)\nprint('CUDA available:', torch.cuda.is_available())\n\n# Select device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', DEVICE)\n\n# Reproducibility: set seeds and deterministic flags\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# Deterministic behavior (slower but reproducible)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Data Loading & Visualization"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Transforms: convert to tensor & normalize to mean=0.1307, std=0.3081 (standard MNIST normalization)\n# These values are commonly used and help with stable training\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# (Optional) light augmentation for robustness; keep disabled for baseline\n# aug_transform = transforms.Compose([\n#     transforms.RandomRotation(degrees=10),\n#     transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n#     transforms.ToTensor(),\n#     transforms.Normalize((0.1307,), (0.3081,))\n# ])\n\n# Download/Load MNIST\nDATA_DIR = './data'\ntrain_dataset = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transform)\ntest_dataset  = datasets.MNIST(root=DATA_DIR, train=False, download=True, transform=transform)\n\n# Create validation split from training data (e.g., 55k train / 5k val)\nVAL_SIZE = 5000\ntrain_size = len(train_dataset) - VAL_SIZE\ntrain_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, VAL_SIZE],\n                                                          generator=torch.Generator().manual_seed(SEED))\n\n# DataLoaders\nBATCH_SIZE = 128\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n# Visualize a grid of samples\nbatch_imgs, batch_labels = next(iter(train_loader))\nimgs = batch_imgs[:16].squeeze(1).numpy()  # (N, 28, 28)\nlabels = batch_labels[:16].numpy()\n\nfig, axes = plt.subplots(4, 4, figsize=(6, 6))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(imgs[i], cmap='gray')\n    ax.set_title(f'label: {labels[i]}')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Model: A Simple CNN"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "class SimpleCNN(nn.Module):\n    # A compact CNN suitable for MNIST (28x28 grayscale).\n    # Architecture:\n    # - Conv(1->32, 3x3) + ReLU\n    # - Conv(32->64, 3x3) + ReLU + MaxPool(2x2)\n    # - Dropout(0.25)\n    # - Flatten\n    # - FC(64*14*14 -> 128) + ReLU\n    # - Dropout(0.5)\n    # - FC(128 -> 10)\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool  = nn.MaxPool2d(2, 2)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        # After two convs + one pool: input 28x28 -> pool halves to 14x14; padding keeps size after convs\n        # conv->conv->pool gives (64, 14, 14)\n        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# Instantiate model\nmodel = SimpleCNN().to(DEVICE)\n\n# Count parameters\nparam_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Model has {param_count:,} trainable parameters')\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Training Utilities"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def accuracy_from_logits(logits, targets):\n    preds = logits.argmax(dim=1)\n    return (preds == targets).float().mean().item()\n\n\ndef train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    for images, labels in loader:\n        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n        running_acc += (outputs.argmax(dim=1) == labels).sum().item()\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n            running_acc += (outputs.argmax(dim=1) == labels).sum().item()\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = running_acc / len(loader.dataset)\n    return epoch_loss, epoch_acc\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Train Loop (Baseline)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Hyperparameters (baseline)\nLR = 1e-3          # learning rate\nWEIGHT_DECAY = 1e-4\nEPOCHS = 10\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n# Optional scheduler to reduce LR over time\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\ntrain_losses, train_accs = [], []\nval_losses, val_accs = [], []\n\nbest_val_acc = 0.0\nbest_state = None\n\nfor epoch in range(1, EPOCHS+1):\n    tr_loss, tr_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n    va_loss, va_acc = evaluate(model, val_loader, criterion, DEVICE)\n    scheduler.step()\n\n    train_losses.append(tr_loss)\n    train_accs.append(tr_acc)\n    val_losses.append(va_loss)\n    val_accs.append(va_acc)\n\n    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n          f\"Train Loss: {tr_loss:.4f}, Train Acc: {tr_acc*100:.2f}% | \"\n          f\"Val Loss: {va_loss:.4f}, Val Acc: {va_acc*100:.2f}%\")\n\n    # Save best model\n    if va_acc > best_val_acc:\n        best_val_acc = va_acc\n        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n\n# Load best model state before final evaluation\nif best_state is not None:\n    model.load_state_dict(best_state)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Training Curves: Loss & Accuracy"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "epochs = range(1, EPOCHS+1)\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(epochs, train_losses, label='Train Loss')\nplt.plot(epochs, val_losses, label='Val Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(epochs, [a*100 for a in train_accs], label='Train Acc')\nplt.plot(epochs, [a*100 for a in val_accs], label='Val Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Epochs')\nplt.legend()\nplt.tight_layout()\nplt.show()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Test Evaluation & Confusion Matrix"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)\nprint(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%')\n\n# Confusion matrix\nnum_classes = 10\ncm = np.zeros((num_classes, num_classes), dtype=int)\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(DEVICE), labels.to(DEVICE)\n        outputs = model(images)\n        preds = outputs.argmax(dim=1)\n        for t, p in zip(labels.view(-1), preds.view(-1)):\n            cm[t.long().item(), p.long().item()] += 1\n\nfig, ax = plt.subplots(figsize=(6,6))\nim = ax.imshow(cm, cmap='Blues')\nax.figure.colorbar(im, ax=ax)\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nax.set_xticks(range(num_classes))\nax.set_yticks(range(num_classes))\nax.set_title('MNIST Confusion Matrix (Test)')\n\n# Annotate counts\nfor i in range(num_classes):\n    for j in range(num_classes):\n        ax.text(j, i, cm[i, j], ha='center', va='center', color='black', fontsize=8)\nplt.tight_layout()\nplt.show()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Save & Load Model"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "SAVE_PATH = 'mnist_cnn.pth'\ntorch.save(model.state_dict(), SAVE_PATH)\nprint(f'Saved best model weights to {SAVE_PATH}')\n\n# Example: re-load\nloaded_model = SimpleCNN().to(DEVICE)\nloaded_model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))\nloaded_model.eval()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. Inference Demo"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Show predictions on a small batch\nimages, labels = next(iter(test_loader))\nimages, labels = images.to(DEVICE), labels.to(DEVICE)\noutputs = loaded_model(images)\npreds = outputs.argmax(dim=1)\n\n# Plot first 16\nimgs = images[:16].cpu().squeeze(1).numpy()\ntrue = labels[:16].cpu().numpy()\nest  = preds[:16].cpu().numpy()\n\nfig, axes = plt.subplots(4, 4, figsize=(6, 6))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(imgs[i], cmap='gray')\n    ax.set_title(f'T:{true[i]} P:{est[i]}', fontsize=10)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11. Hyperparameter Tuning Notes\n\n**Goal:** Achieve strong validation/test accuracy with efficient training.\n\n### Key Hyperparameters & Practical Ranges\n- **Learning Rate (LR):** Start with `1e-3` for Adam. Try `{3e-4, 1e-3, 3e-3}`. For SGD, begin around `0.05`–`0.2` with momentum.\n- **Batch Size:** MNIST trains well with `64`–`256`. Larger batches speed up on GPU but may need LR adjustment (linear scaling rule).\n- **Weight Decay (L2):** Helps generalization. Try `{0, 1e-5, 1e-4, 5e-4}`.\n- **Dropout:** `0.25`–`0.5` is typical; lower for small models if underfitting.\n- **Optimizer:** `Adam` is a reliable baseline; `SGD(momentum=0.9)` can match/beat Adam with tuned LR and decay.\n- **LR Schedule:** StepLR (decay by 0.5–0.1 every 5 epochs) or CosineAnnealing; improves final accuracy.\n- **Data Augmentation:** Light rotations (±10°) and translations (≤10%) can add robustness; avoid heavy transforms that distort digits.\n\n### Tuning Strategy (Time-Effective)\n1. **Baseline run** (as provided) to establish reference accuracy.\n2. **LR sweep**: Keep other params fixed; test 3–5 LRs for 3–5 epochs, pick best.\n3. **Regularization sweep**: Try a couple of weight_decay and dropout combinations.\n4. **Optimizer comparison**: Adam vs SGD+momentum with tuned LR.\n5. **Schedule on**: Enable StepLR/Cosine once core hyperparams are set.\n\n### Diagnostics\n- If **train acc >> val acc**: decrease LR, increase weight_decay or dropout, add light augmentation.\n- If **train acc ≈ val acc but both low**: increase capacity (more filters), train longer, or try a slightly higher LR.\n- If **loss plateaus early**: try LR warmup or restart with higher LR.\n\n### Recommended Defaults (MNIST)\n- Adam, `lr=1e-3`, `weight_decay=1e-4`\n- Batch size `128`\n- 10–15 epochs with StepLR (step_size=5, gamma=0.5)\n- Optional light augmentation for extra robustness\n\n> The MNIST dataset and transforms are provided by `torchvision.datasets.MNIST` and `torchvision.transforms`. See the official docs: https://docs.pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 12. (Optional) Tiny Hyperparameter Sweep"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# WARNING: This will train multiple short runs and can take extra time.\n# It runs 2 epochs per config to quickly compare validation accuracy.\n\nfrom copy import deepcopy\n\ndef run_short_experiment(lr, weight_decay, epochs=2):\n    model = SimpleCNN().to(DEVICE)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    best_va = 0.0\n    for ep in range(epochs):\n        train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n        va_loss, va_acc = evaluate(model, val_loader, criterion, DEVICE)\n        best_va = max(best_va, va_acc)\n    return best_va\n\ncandidates = [\n    {\"lr\": 3e-4, \"wd\": 0.0},\n    {\"lr\": 1e-3, \"wd\": 1e-5},\n    {\"lr\": 1e-3, \"wd\": 1e-4},\n    {\"lr\": 3e-3, \"wd\": 1e-4},\n]\n\nresults = []\nfor cfg in candidates:\n    acc = run_short_experiment(cfg[\"lr\"], cfg[\"wd\"], epochs=2)\n    results.append({\"lr\": cfg[\"lr\"], \"wd\": cfg[\"wd\"], \"val_acc\": acc})\n    print(f\"Config lr={cfg['lr']:.1e}, wd={cfg['wd']:.1e} -> best val acc: {acc*100:.2f}%\")\n\n# Pick best\nbest = max(results, key=lambda r: r[\"val_acc\"]) if results else None\nprint('Best (short run):', best)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 13. Appendix: Notes & Tips\n\n- **Reproducibility:** We set seeds and deterministic flags; exact reproducibility may still vary across hardware and CUDA versions.\n- **Runtime:** On CPU, expect ~1–5 minutes for 10 epochs; on GPU, seconds. Times vary by hardware.\n- **Model Capacity:** You can add another conv+pool block or increase filters (e.g., 64→128) to push accuracy closer to ~99%.\n- **Evaluation Protocol:** Use a held-out validation split (as we did) to tune hyperparameters, then report final **test** performance with best weights.\n- **Saving Artifacts:** Models (`.pth`) and plots can be saved to disk for reporting.\n"
    }
  ]
}