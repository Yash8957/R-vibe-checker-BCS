{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n# Stock Price Prediction with Sequential Models (PyTorch)\n\nThis notebook demonstrates **stock price prediction** using sequential neural networks — **LSTM** and **GRU** — implemented in PyTorch. \n\nIt supports two data sources:\n\n1. **Yahoo Finance via `yfinance`** (set `USE_YFINANCE=True` below).  \n2. **A local CSV** (default; the synthetic file shipped with this notebook: `synthetic_stock_AAPL_like.csv`).\n\n> **Note:** If you're running offline or don't have `yfinance` installed, the notebook will automatically fall back to the included synthetic dataset.\n\nWe: \n- load OHLCV data;  \n- prepare windowed sequences;  \n- train **both** LSTM and GRU models;  \n- evaluate with RMSE/MAE/MAPE;  \n- plot predicted vs actual closes.\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n### References\n- `yfinance` project page: https://pypi.org/project/yfinance/  \n- A PyTorch LSTM stock prediction walkthrough (community resources exist across blogs & repos).  \n\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Imports\nimport os\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nrcParams['figure.figsize'] = (12, 5)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Try to import yfinance if available\ntry:\n    import yfinance as yf\n    HAS_YF = True\nexcept Exception:\n    HAS_YF = False\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using device:', DEVICE)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Configuration\nTICKER = 'AAPL'\nSTART = '2015-01-01'\nEND = datetime.today().strftime('%Y-%m-%d')\nUSE_YFINANCE = False  # set True to fetch live data if online\nLOCAL_CSV = 'synthetic_stock_AAPL_like.csv'  # default synthetic dataset shipped with this project\nTARGET_COL = 'Close'\nSEQ_LEN = 60\nBATCH_SIZE = 64\nEPOCHS = 20\nLR = 1e-3\nVAL_SPLIT = 0.1\nTEST_SPLIT = 0.1\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Data loading\nif USE_YFINANCE and HAS_YF:\n    print('Fetching data from Yahoo Finance for', TICKER)\n    df = yf.download(tickers=TICKER, start=START, end=END)\n    df = df.reset_index()\n    df['Ticker'] = TICKER\n    df.rename(columns={'Adj Close':'Adj Close'}, inplace=True)\nelse:\n    print('Using local CSV:', LOCAL_CSV)\n    df = pd.read_csv(LOCAL_CSV)\n    df['Date'] = pd.to_datetime(df['Date'])\n\n# Sort by date just in case\ndf = df.sort_values('Date').reset_index(drop=True)\nprint('Rows:', len(df), 'Date range:', df['Date'].min(), '->', df['Date'].max())\n\n# Basic sanity\nfor c in ['Open','High','Low','Close','Adj Close','Volume']:\n    assert c in df.columns, f\"Missing column {c}\"\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Feature scaling\nfeatures = ['Open','High','Low','Close','Volume']\nscaler = MinMaxScaler()\nscaled = scaler.fit_transform(df[features])\n\n# We predict next-day Close (scaled)\nclose_idx = features.index('Close')\nseries = scaled[:, close_idx].reshape(-1, 1)\n\n# Create sequences\nX, y = [], []\nfor i in range(len(series) - SEQ_LEN):\n    X.append(scaled[i:i+SEQ_LEN, :])\n    y.append(series[i+SEQ_LEN])\nX = np.array(X)\ny = np.array(y)\n\n# Train/Val/Test split by time\nn = len(X)\ntest_size = int(n * TEST_SPLIT)\nval_size = int(n * VAL_SPLIT)\ntrain_size = n - val_size - test_size\nX_train, y_train = X[:train_size], y[:train_size]\nX_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\nX_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n\nprint(f'Train/Val/Test: {len(X_train)}/{len(X_val)}/{len(X_test)} samples')\n\nclass SeqDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(SeqDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(SeqDataset(X_val, y_val), batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(SeqDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Define LSTM and GRU models\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n        self.fc = nn.Linear(hidden_size, 1)\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        # take last time-step\n        out = out[:, -1, :]\n        return self.fc(out)\n\nclass GRUModel(nn.Module):\n    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):\n        super().__init__()\n        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n        self.fc = nn.Linear(hidden_size, 1)\n    def forward(self, x):\n        out, _ = self.gru(x)\n        out = out[:, -1, :]\n        return self.fc(out)\n\ninput_size = len(features)\nlstm_model = LSTMModel(input_size).to(DEVICE)\ngru_model = GRUModel(input_size).to(DEVICE)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Training utilities\n\ndef train_model(model, train_loader, val_loader, epochs=EPOCHS, lr=LR):\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    best_val = float('inf')\n    history = {'train_loss': [], 'val_loss': []}\n    for epoch in range(1, epochs+1):\n        model.train()\n        train_losses = []\n        for xb, yb in train_loader:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            pred = model(xb)\n            loss = criterion(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n        # validation\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n                pred = model(xb)\n                loss = criterion(pred, yb)\n                val_losses.append(loss.item())\n        avg_train = float(np.mean(train_losses)) if train_losses else float('nan')\n        avg_val = float(np.mean(val_losses)) if val_losses else float('nan')\n        history['train_loss'].append(avg_train)\n        history['val_loss'].append(avg_val)\n        if avg_val < best_val:\n            best_val = avg_val\n            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n        if epoch % 5 == 0:\n            print(f'Epoch {epoch}/{epochs} - train {avg_train:.4f} - val {avg_val:.4f}')\n    # restore best state\n    model.load_state_dict(best_state)\n    return history\n\nprint('Training LSTM...')\nlstm_hist = train_model(lstm_model, train_loader, val_loader)\nprint('Training GRU...')\ngru_hist = train_model(gru_model, train_loader, val_loader)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Evaluate on test set and invert scaling\n\ndef evaluate(model, loader):\n    model.eval()\n    preds, targets = [], []\n    with torch.no_grad():\n        for xb, yb in loader:\n            xb = xb.to(DEVICE)\n            pred = model(xb).cpu().numpy()\n            preds.append(pred)\n            targets.append(yb.numpy())\n    preds = np.concatenate(preds, axis=0)\n    targets = np.concatenate(targets, axis=0)\n    # invert scaling for Close\n    # build array with preds at Close index and zeros elsewhere to use scaler.inverse_transform\n    inv_preds = []\n    inv_targets = []\n    for p in preds:\n        row = np.zeros((1, len(features)))\n        row[0, close_idx] = p\n        inv_p = scaler.inverse_transform(row)[0, close_idx]\n        inv_preds.append(inv_p)\n    for t in targets:\n        row = np.zeros((1, len(features)))\n        row[0, close_idx] = t\n        inv_t = scaler.inverse_transform(row)[0, close_idx]\n        inv_targets.append(inv_t)\n    inv_preds = np.array(inv_preds).reshape(-1)\n    inv_targets = np.array(inv_targets).reshape(-1)\n    rmse = math.sqrt(mean_squared_error(inv_targets, inv_preds))\n    mae = mean_absolute_error(inv_targets, inv_preds)\n    mape = np.mean(np.abs((inv_targets - inv_preds) / (inv_targets + 1e-8))) * 100\n    return inv_preds, inv_targets, rmse, mae, mape\n\nlstm_preds, lstm_true, lstm_rmse, lstm_mae, lstm_mape = evaluate(lstm_model, test_loader)\ngru_preds, gru_true, gru_rmse, gru_mae, gru_mape = evaluate(gru_model, test_loader)\n\nprint(f'LSTM - RMSE: {lstm_rmse:.4f}, MAE: {lstm_mae:.4f}, MAPE: {lstm_mape:.2f}%')\nprint(f'GRU  - RMSE: {gru_rmse:.4f}, MAE: {gru_mae:.4f}, MAPE: {gru_mape:.2f}%')\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Plot predictions vs actual (test portion)\nplt.figure(figsize=(12,5))\nplt.plot(lstm_true, label='Actual (Close)')\nplt.plot(lstm_preds, label='LSTM Predicted')\nplt.plot(gru_preds, label='GRU Predicted')\nplt.title('Predicted vs Actual Closing Prices (Test set)')\nplt.xlabel('Time index in test set')\nplt.ylabel('Price')\nplt.legend()\nplt.grid(True)\nplt.show()\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "\n# Save models and predictions\nos.makedirs('artifacts', exist_ok=True)\ntorch.save(lstm_model.state_dict(), 'artifacts/lstm_model.pt')\ntorch.save(gru_model.state_dict(), 'artifacts/gru_model.pt')\nnp.save('artifacts/lstm_preds.npy', lstm_preds)\nnp.save('artifacts/gru_preds.npy', gru_preds)\nnp.save('artifacts/test_true.npy', lstm_true)\nprint('Artifacts saved to ./artifacts')\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n### Notes & Next Steps\n- Try toggling `USE_YFINANCE=True` to fetch real data for any ticker (requires internet and `yfinance`).  \n- Experiment with feature engineering: add technical indicators (SMA, EMA, RSI), macro data, or news sentiment for richer inputs.  \n- Adjust hyperparameters (sequence length, hidden sizes, layers) and consider **early stopping** or learning rate schedules.  \n- For production-grade models, evaluate **walk-forward validation**, **data leakage checks**, and **robust scaling** (e.g., `RobustScaler`).\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}