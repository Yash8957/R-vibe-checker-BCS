{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c724022",
   "metadata": {},
   "source": [
    "# ANN for Customer Churn (PyTorch)\n",
    "\n",
    "**Dataset**: Place `Churn_Modelling.csv` next to this notebook.\n",
    "\n",
    "**Goal**: Build, tune, and evaluate a feedforward ANN for binary classification (churn).\n",
    "\n",
    "**References**:\n",
    "- Kaggle dataset: Deep Learning A-Z ANN (Churn Modelling)\n",
    "- Article: Building an ANN with PyTorch (Jillani Soft Tech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdad4fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import os, random, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except Exception:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70595715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_PATH = \"Churn_Modelling.csv\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(\"Place 'Churn_Modelling.csv' in this folder or update DATA_PATH.\")\n",
    "raw_df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", raw_df.shape)\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ebd71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA\n",
    "print(\"Columns:\", list(raw_df.columns))\n",
    "print(\"Class distribution (Exited):\n",
    "\", raw_df['Exited'].value_counts())\n",
    "print(\"Missing values:\n",
    "\", raw_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02795f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "df = raw_df.drop(columns=['RowNumber','CustomerId','Surname'])\n",
    "df['Gender'] = df['Gender'].map({'Male':1,'Female':0}).astype(int)\n",
    "df = pd.get_dummies(df, columns=['Geography'], drop_first=True)\n",
    "X = df.drop(columns=['Exited']).values.astype(np.float32)\n",
    "y = df['Exited'].values.astype(np.int64)\n",
    "\n",
    "N = X.shape[0]\n",
    "idx = np.arange(N); np.random.shuffle(idx)\n",
    "train_end = int(0.6*N); val_end = int(0.8*N)\n",
    "train_idx, val_idx, test_idx = idx[:train_end], idx[train_end:val_end], idx[val_end:]\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val, y_val = X[val_idx], y[val_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "mu = X_train.mean(axis=0); sigma = X_train.std(axis=0) + 1e-8\n",
    "X_train = (X_train - mu)/sigma\n",
    "X_val = (X_val - mu)/sigma\n",
    "X_test = (X_test - mu)/sigma\n",
    "\n",
    "X_train_t, y_train_t = torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
    "X_val_t,   y_val_t   = torch.from_numpy(X_val),   torch.from_numpy(y_val)\n",
    "X_test_t,  y_test_t  = torch.from_numpy(X_test),  torch.from_numpy(y_test)\n",
    "\n",
    "class ChurnDataset(Dataset):\n",
    "    def __init__(self,X,y): self.X, self.y = X, y\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self,i): return self.X[i], self.y[i]\n",
    "\n",
    "train_ds, val_ds, test_ds = ChurnDataset(X_train_t,y_train_t), ChurnDataset(X_val_t,y_val_t), ChurnDataset(X_test_t,y_test_t)\n",
    "print(f\"Train/Val/Test: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4398722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class ANNNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []; d = input_dim\n",
    "        for _ in range(num_layers):\n",
    "            layers += [nn.Linear(d, hidden_dim), nn.ReLU()]\n",
    "            if dropout>0: layers += [nn.Dropout(dropout)]\n",
    "            d = hidden_dim\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        self.head = nn.Linear(d, 1)\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x)).view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7f03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities\n",
    "from math import isfinite\n",
    "\n",
    "def loader(ds, bs=64, shuffle=True):\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle)\n",
    "\n",
    "def evaluate(model, dl):\n",
    "    model.eval(); logits=[], targets=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in dl:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            l = model(xb)\n",
    "            logits.append(l.cpu().numpy()); targets.append(yb.cpu().numpy())\n",
    "    import numpy as np\n",
    "    L = np.concatenate(logits); T = np.concatenate(targets)\n",
    "    P = 1/(1+np.exp(-L)); pred = (P>=0.5).astype(int)\n",
    "    acc = (pred==T).mean(); metrics={'accuracy':float(acc)}\n",
    "    if SKLEARN_AVAILABLE:\n",
    "        try:\n",
    "            metrics['roc_auc'] = float(roc_auc_score(T,P))\n",
    "        except Exception: pass\n",
    "    return metrics\n",
    "\n",
    "def train_model(hp):\n",
    "    model = ANNNet(X_train.shape[1], hp.get('hidden_dim',32), hp.get('num_layers',1), hp.get('dropout',0.2)).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=hp.get('lr',1e-3), weight_decay=hp.get('weight_decay',0.0))\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "    bs = hp.get('batch_size',64); epochs = hp.get('epochs',30); patience = hp.get('patience',5)\n",
    "    dl_tr, dl_va = loader(train_ds, bs), loader(val_ds, bs, shuffle=False)\n",
    "    best_acc=-1; best=None; hist={'train_loss':[], 'val_loss':[], 'val_acc':[]}\n",
    "    noimp=0\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train(); tot=0\n",
    "        for xb,yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.float().to(DEVICE)\n",
    "            opt.zero_grad(); logit = model(xb); loss = crit(logit,yb); loss.backward(); opt.step()\n",
    "            tot += loss.item()*xb.size(0)\n",
    "        tr_loss = tot/len(train_ds)\n",
    "        # val\n",
    "        model.eval(); vt=0\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.float().to(DEVICE)\n",
    "                l = crit(model(xb), yb); vt += l.item()*xb.size(0)\n",
    "        va_loss = vt/len(val_ds); va_metrics = evaluate(model, dl_va)\n",
    "        hist['train_loss'].append(tr_loss); hist['val_loss'].append(va_loss); hist['val_acc'].append(va_metrics['accuracy'])\n",
    "        if va_metrics['accuracy']>best_acc:\n",
    "            best_acc = va_metrics['accuracy']; best={'state':model.state_dict(), 'hp':hp, 'ep':ep}; noimp=0\n",
    "        else:\n",
    "            noimp+=1\n",
    "        if noimp>=patience: print(f\"Early stop @epoch {ep}\"); break\n",
    "        if ep%5==0: print(f\"Epoch {ep}: tr_loss={tr_loss:.4f} va_loss={va_loss:.4f} va_acc={va_metrics['accuracy']:.4f}\")\n",
    "    if best: model.load_state_dict(best['state'])\n",
    "    return model, hist, best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small random hyperparameter search\n",
    "space = {\n",
    "    'lr':[1e-3,5e-4,1e-4],\n",
    "    'hidden_dim':[16,32,64],\n",
    "    'num_layers':[1,2],\n",
    "    'dropout':[0.0,0.2,0.5],\n",
    "    'batch_size':[32,64,128],\n",
    "    'weight_decay':[0.0,1e-5,1e-4]\n",
    "}\n",
    "trials=[]\n",
    "for _ in range(10):\n",
    "    hp = {k: random.choice(v) for k,v in space.items()}\n",
    "    hp['epochs']=30; hp['patience']=5\n",
    "    model,hist,va = train_model(hp)\n",
    "    trials.append({'hp':hp,'va':float(va),'hist':hist})\n",
    "    print('Trial:', hp, '| val_acc=', va)\n",
    "\n",
    "best = max(trials, key=lambda d:d['va'])\n",
    "print('Best hparams:', best['hp'])\n",
    "print('Best val acc:', best['va'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training (train+val) and test evaluation\n",
    "X_trval = np.vstack([X_train, X_val]).astype(np.float32)\n",
    "y_trval = np.concatenate([y_train, y_val]).astype(np.int64)\n",
    "X_trval_t = torch.from_numpy(X_trval); y_trval_t = torch.from_numpy(y_trval)\n",
    "trval_ds = ChurnDataset(X_trval_t,y_trval_t)\n",
    "\n",
    "bhp = best['hp'].copy(); bhp['epochs']=40; bhp['patience']=6\n",
    "model,hist,_ = train_model(bhp)\n",
    "\n",
    "# Test\n",
    "dl_te = DataLoader(test_ds, batch_size=bhp.get('batch_size',64), shuffle=False)\n",
    "metrics = evaluate(model, dl_te)\n",
    "print('Test metrics:', metrics)\n",
    "\n",
    "if SKLEARN_AVAILABLE:\n",
    "    model.eval(); probs=[]; targets=[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in dl_te:\n",
    "            xb = xb.to(DEVICE)\n",
    "            p = torch.sigmoid(model(xb)).cpu().numpy()\n",
    "            probs.append(p); targets.append(yb.cpu().numpy())\n",
    "    import numpy as np\n",
    "    P = np.concatenate(probs); T = np.concatenate(targets)\n",
    "    preds = (P>=0.5).astype(int)\n",
    "    print('\n",
    "Confusion Matrix:\n",
    "', confusion_matrix(T,preds))\n",
    "    print('\n",
    "Classification Report:\n",
    "', classification_report(T,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and preprocessing\n",
    "import json, os\n",
    "ARTIFACT_DIR='models'; os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "artifact={'train_mean':mu.tolist(),'train_std':sigma.tolist(),'best_hparams':best['hp']}\n",
    "\n",
    "torch.save({'state_dict':model.state_dict(),'artifact':artifact}, os.path.join(ARTIFACT_DIR,'best_ann.pth'))\n",
    "with open(os.path.join(ARTIFACT_DIR,'preprocessing.json'),'w') as f: json.dump(artifact,f,indent=2)\n",
    "print('Artifacts saved to', ARTIFACT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ffe30",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Notes\n",
    "- lr: start 1e-3 for Adam; reduce if unstable.\n",
    "- hidden_dim/num_layers: increase capacity if underfitting; reduce to avoid overfitting.\n",
    "- dropout: 0.2–0.5 helps regularize.\n",
    "- batch_size: 64 default; larger batches smooth gradients.\n",
    "- weight_decay: 1e-5–1e-4 adds L2 regularization.\n",
    "- early stopping: patience 5–6 based on validation accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
